{"models": {"typos_correction": {"d798e898-c6b2-4e39-809f-f502571584e8": {"datasets": [[]], "series": 0.0, "description": "Model that suggests fixes to correct typos.", "references": [[]], "extra": {"Proba of >1 typo in a typo-ed word": 0, "Vocabulary size": 5000, "Fasttext train size": 10000000, "Frequencies size": 50000, "Train size": 50000}, "created_at": "2019-03-18 14:16:26", "tags": ["typos_correction"], "vendor": "source{d}", "environment": {"platform": "Linux-4.15.15-coreos-x86_64-with-Ubuntu-16.04-xenial", "packages": [["ConfigArgParse", "0.14.0"], ["Jinja2", "2.10"], ["MarkupSafe", "1.0"], ["Pillow", "5.4.1"], ["Pillow-SIMD", "5.1.1.post0"], ["PyStemmer", "1.3.0"], ["PyYAML", "3.12"], ["Pygments", "2.3.1"], ["Pympler", "0.6"], ["SQLAlchemy", "1.2.18"], ["SQLAlchemy-Utils", "0.33.11"], ["asdf", "2.3.2"], ["backcall", "0.1.0"], ["bblfsh", "2.12.7"], ["boto", "2.49.0"], ["boto3", "1.9.98"], ["botocore", "1.12.98"], ["cachetools", "2.1.0"], ["certifi", "2018.4.16"], ["cffi", "1.5.2"], ["chardet", "3.0.4"], ["clint", "0.5.1"], ["cycler", "0.10.0"], ["decorator", "4.3.0"], ["dulwich", "0.19.11"], ["fasttext", "0.8.22"], ["gensim", "3.7.1"], ["grpcio", "1.18.0"], ["grpcio-tools", "1.18.0"], ["humanfriendly", "4.17"], ["humanize", "0.5.1"], ["idna", "2.6"], ["ipykernel", "4.8.2"], ["ipython", "6.3.1"], ["ipython-genutils", "0.2.0"], ["ipywidgets", "7.2.1"], ["jedi", "0.12.0"], ["jmespath", "0.9.3"], ["jsonschema", "2.6.0"], ["jupyter-client", "5.2.3"], ["jupyter-core", "4.4.0"], ["lookout-sdk", "0.4.1"], ["lookout-sdk-ml", "0.15.0"], ["lookout-style", "0.1.1"], ["lz4", "2.1.6"], ["matplotlib", "2.2.2"], ["modelforge", "0.11.1"], ["numpy", "1.14.2"], ["pandas", "0.22.0"], ["parso", "0.2.0"], ["pexpect", "4.5.0"], ["pickleshare", "0.7.4"], ["pip", "19.0.3"], ["prompt-toolkit", "1.0.15"], ["protobuf", "3.7.0"], ["psycopg2-binary", "2.7.7"], ["ptyprocess", "0.5.2"], ["pygtrie", "2.3"], ["pyparsing", "2.2.0"], ["python-dateutil", "2.7.2"], ["pytz", "2018.4"], ["pyzmq", "17.0.0"], ["requests", "2.18.4"], ["requirements-parser", "0.2.0"], ["scikit-learn", "0.20.1"], ["scipy", "1.0.1"], ["semantic-version", "2.6.0"], ["setuptools", "40.8.0"], ["simplegeneric", "0.8.1"], ["six", "1.11.0"], ["smart-open", "1.8.0"], ["sourced-ml", "0.8.2"], ["spdx", "2.5.0"], ["stringcase", "1.2.0"], ["tornado", "5.0.2"], ["tqdm", "4.31.1"], ["traitlets", "4.3.2"], ["urllib3", "1.22"], ["wcwidth", "0.1.7"], ["xgboost", "0.72.1"], ["xxhash", "1.3.0"]], "python": "3.5.2 (default, Nov 23 2017, 16:37:01) [GCC 5.4.0 20160609]"}, "metrics": {"accuracy": 0.9128, "recall": 0.8256, "precision": 1.0, "f1": 0.9044697633654688}, "url": "https://storage.googleapis.com/models.cdn.sourced.tech/models%2Ftypos_correction%2Fd798e898-c6b2-4e39-809f-f502571584e8.asdf", "license": "ODbL-1.0", "dependencies": [], "size": "11.2 MB", "code": "from lookout.style.typos.corrector import TyposCorrector\ncorrector = TyposCorrector().load(%s)\nprint(\"Corrector configuration:\\n\", corrector.dump())", "version": [1, 0, 0]}}, "topics": {"c70a7514-9257-4b33-b468-27a8588d4dfa": {"dependencies": ["f64bacd4-67fb-4c64-8382-399a8e7db52a"], "description": "Generated from 2 million GitHub repositories in October 2016.", "version": [0, 3, 0], "parent": "", "created_at": "2017-09-18 12:27:56.074233", "size": "95.1 MB", "code": "from sourced.ml.models import Topics\ntopics = Topics().load(%s)\nprint(\"Number of topics:\", len(topics))\nprint(\"Number of repositories:\", len(topics.tokens))", "extra": {"Number of topics": "320", "Number of tokens": "2,015,336", "Data collection date": "October 2016"}, "license": ["", "undecided"], "url": "https://storage.googleapis.com/models.cdn.sourced.tech/models%2Ftopics%2Fc70a7514-9257-4b33-b468-27a8588d4dfa.asdf", "references": [["Topic modeling of public repositories at scale using names in source code", "https://arxiv.org/abs/1704.00135"]]}}, "bow": {"da8c5dee-b285-4d55-8913-a5209f716564": {"dependencies": ["55215392-36fc-43e5-b277-500f5b68d0c6"], "description": "Bags of features, extracted in july 2018 from 7.8 million distinct files from PGA (taking only the `HEAD` commit), using all implemented extractors in `sourced.ml` at the time (`identifiers`, `literals`, `graphlets`, `children`, `node2vec` and `uast2seq`) and all languages parsable by Babelfish (Go, Java, Python, Bash, JavaScript and Ruby). This was done to try to use [apollo](https://github.com/src-d/apollo) at scale. We hit `scipy.sparse` limits while trying to merge sparse matrices for all bags, so this is only one of three `BOW` model holding bags.", "version": [1, 0, 0], "parent": "", "created_at": "2018-07-17 09:43:05.498579", "size": "25.8 GB", "code": "from sourced.ml.models import BOW\nbow = BOW().load(%s)\nprint(\"Number of documents:\", len(bow))\nprint(\"Number of tokens:\", len(bow.tokens))", "extra": {"Other parts": "[694c20a0-9b96-4444-80ae-f2fa5bd1395b](694c20a0-9b96-4444-80ae-f2fa5bd1395b.md) and [1e0deee4-7dc1-400f-acb6-74c0f4aec471](1e0deee4-7dc1-400f-acb6-74c0f4aec471.md)", "Number of distinct documents (files)": "3,493,288", "Number of distinct features": "6,194,874", "Data collection date": "July 2018"}, "license": ["", "none"], "url": "https://storage.googleapis.com/models.cdn.sourced.tech/models%2Fbow%2Fda8c5dee-b285-4d55-8913-a5209f716564.asdf", "references": []}, "1e3da42a-28b6-4b33-94a2-a5671f4102f4": {"dependencies": ["f64bacd4-67fb-4c64-8382-399a8e7db52a"], "description": "Bags of identifiers generated from 140,000 most starred projects on GitHub in October 2016 - ~112k after deduplication.", "version": [1, 0, 0], "parent": "", "created_at": "2017-06-19 09:16:08.942880", "size": "380.8 MB", "code": "from sourced.ml.models import BOW\nbow = BOW().load(%s)\nprint(\"Number of documents:\", len(bow))\nprint(\"Number of tokens:\", len(bow.tokens))", "extra": {"Number of repositories": "112,273", "Number of (sub)tokens": "999,424", "Data collection date": "October 2016"}, "license": ["", "undecided"], "url": "https://storage.googleapis.com/models.cdn.sourced.tech/models%2Fbow%2F1e3da42a-28b6-4b33-94a2-a5671f4102f4.asdf", "references": [["Similarity of GitHub Repositories by Source Code Identifiers", "http://vmarkovtsev.github.io/techtalks-2017-moscow/#"]]}, "1e0deee4-7dc1-400f-acb6-74c0f4aec471": {"dependencies": ["55215392-36fc-43e5-b277-500f5b68d0c6"], "parent": "", "description": "Bags of features, extracted in july 2018 from 7.8 million distinct files from PGA (taking only the `HEAD` commit), using all implemented extractors in `sourced.ml` at the time (`identifiers`, `literals`, `graphlets`, `children`, `node2vec` and `uast2seq`) and all languages parsable by Babelfish (Go, Java, Python, Bash, JavaScript and Ruby). This was done to try to use [apollo](https://github.com/src-d/apollo) at scale. We hit `scipy.sparse` limits while trying to merge sparse matrices for all bags, so this is only one of three `BOW` model holding bags.", "version": [1, 0, 0], "extra": {"Other parts": "[694c20a0-9b96-4444-80ae-f2fa5bd1395b](694c20a0-9b96-4444-80ae-f2fa5bd1395b.md) and [da8c5dee-b285-4d55-8913-a5209f716564](da8c5dee-b285-4d55-8913-a5209f716564.md)", "Number of distinct documents (files)": "864,458", "Number of distinct features": "6,194,874", "Data collection date": "July 2018"}, "created_at": "2018-07-17 10:16:51.105969", "size": "5.9 GB", "code": "from sourced.ml.models import BOW\nbow = BOW().load(%s)\nprint(\"Number of documents:\", len(bow))\nprint(\"Number of tokens:\", len(bow.tokens))", "license": ["", "none"], "url": "https://storage.googleapis.com/models.cdn.sourced.tech/models%2Fbow%2F1e0deee4-7dc1-400f-acb6-74c0f4aec471.asdf", "references": []}, "694c20a0-9b96-4444-80ae-f2fa5bd1395b": {"dependencies": ["55215392-36fc-43e5-b277-500f5b68d0c6"], "parent": "", "description": "Bags of features, extracted in july 2018 from 7.8 million distinct files from PGA (taking only the `HEAD` commit), using all implemented extractors in `sourced.ml` at the time (`identifiers`, `literals`, `graphlets`, `children`, `node2vec` and `uast2seq`) and all languages parsable by Babelfish (Go, Java, Python, Bash, JavaScript and Ruby). This was done to try to use [apollo](https://github.com/src-d/apollo) at scale. We hit `scipy.sparse` limits while trying to merge sparse matrices for all bags, so this is only one of three `BOW` model holding bags.", "version": [1, 0, 0], "extra": {"Other parts": "[da8c5dee-b285-4d55-8913-a5209f716564](da8c5dee-b285-4d55-8913-a5209f716564.md) and [1e0deee4-7dc1-400f-acb6-74c0f4aec471](1e0deee4-7dc1-400f-acb6-74c0f4aec471.md)", "Number of distinct documents (files)": "3,512,171", "Number of distinct features": "6,194,874", "Data collection date": "July 2018"}, "created_at": "2018-07-17 10:28:56.243131", "size": "26.0 GB", "references": [], "code": "from sourced.ml.models import BOW\nbow = BOW().load(%s)\nprint(\"Number of documents:\", len(bow))\nprint(\"Number of tokens:\", len(bow.tokens))", "url": "https://storage.googleapis.com/models.cdn.sourced.tech/models%2Fbow%2F694c20a0-9b96-4444-80ae-f2fa5bd1395b.asdf", "license": ["", "none"]}}, "docfreq": {"f64bacd4-67fb-4c64-8382-399a8e7db52a": {"dependencies": [], "description": "5.7 million source code identifiers, extracted in october 2016 from all repositories we cloned - 10 million after de-duplication. Standard processing: splitting, stemming - as given in the paper. The document frequency here refers to the frequency of identifiers per repository.", "version": [1, 0, 0], "parent": "", "created_at": "2017-06-19 09:59:14.766638", "size": "24.3 MB", "code": "from sourced.ml.models import DocumentFrequencies\ndf = DocumentFrequencies().load(%s)\nprint(\"Number of tokens:\", len(df))", "extra": {"Number of repositories": "112,273", "Number of (sub)tokens": "5,720,096", "Data collection date": "October 2016"}, "license": ["", "undecided"], "url": "https://storage.googleapis.com/models.cdn.sourced.tech/models%2Fdocfreq%2Ff64bacd4-67fb-4c64-8382-399a8e7db52a.asdf", "references": []}, "55215392-36fc-43e5-b277-500f5b68d0c6": {"dependencies": [], "description": "Bags of features, extracted in july 2018 from 7.8 million distinct files from PGA (taking only the `HEAD` commit), using all implemented extractors in `sourc    ed.ml` at the time (`identifiers`, `literals`, `graphlets`, `children`, `node2vec` and `uast2seq`) and all languages parsable by Babelfish (Go, Java, Python, Bash, JavaScript and Ruby). The document frequency here refers to the frequency of each feature across all documents (we only kept features that appeared at least 5 times).", "version": [1, 0, 0], "parent": "", "created_at": "2018-06-20 14:51:45.469503", "size": "69.9 MB", "code": "from sourced.ml.models import OrderedDocumentFrequencies\ndf = OrderedDocumentFrequencies().load(%s)\nprint(\"Number of documents:\", len(df))", "extra": {"Number of distinct documents (files)": "7,873,334", "Number of distinct features": "6,194,874", "Data collection date": "July 2018"}, "license": ["", "none"], "url": "https://storage.googleapis.com/models.cdn.sourced.tech/models%2Fdocfreq%2F55215392-36fc-43e5-b277-500f5b68d0c6.asdf", "references": []}}, "id2vec": {"92609e70-f79c-46b5-8419-55726e873cfc": {"dependencies": [], "description": "Generated from 140,000 most starred projects on GitHub in October 2016. Legacy pipeline, no splitting and stemming, later converted with quality loss.", "version": [1, 0, 0], "parent": "", "created_at": "2017-06-18 17:37:06.255615", "size": "1.1 GB", "code": "from sourced.ml.models import Id2Vec\nid2vec = Id2Vec().load(%s)\nprint(\"Number of tokens:\", len(id2vec))", "extra": {"Number of repositories": "112,273", "Number of (sub)tokens": "5,720,096", "Data collection date": "October 2016"}, "license": ["", "undecided"], "url": "https://storage.googleapis.com/models.cdn.sourced.tech/models%2Fid2vec%2F92609e70-f79c-46b5-8419-55726e873cfc.asdf", "references": [["Source code identifier embeddings", "https://blog.sourced.tech/post/id2vec/"]]}, "3467e9ca-ec11-444a-ba27-9fa55f5ee6c1": {"dependencies": [], "parent": "", "description": "A little under 1M     identifier embeddings, generated for identifiers extracted from half of PGA in June 2018. New pipeline was used, with splitting and stemming of identifiers, the full descriptio    n can be found in the \"Algorithms\" section of the [sourced.ml](https://github.com/src-d/ml) repository.", "version": [1, 0, 0], "extra": {"Size of each embedding": "300", "Number of tokens": "9    99,424", "Data collection date": "June 2018"}, "created_at": "2018-07-19 13:14:53.000621", "size": "1.2 GB", "code": "from sourced.ml.models import Id2Vec\nid2vec = Id2Vec().load(%s)\nprint(\"Number of tokens:\", len(id2vec))", "references": [["Source code identifier embeddings", "https://blog.sourced.tech/post/id2vec/"]], "url": "https://storage.googleapis.com/models.cdn.sourced.tech/models%2Fid2vec%2F3467e9ca-ec11-444a-ba27-9fa55f5ee6c1.asdf", "license": ["", "none"]}}}, "meta": {"typos_correction": {"code": "from lookout.style.typos.corrector import TyposCorrector\ncorrector = TyposCorrector().load(%s)\nprint(\"Corrector configuration:\\n\", corrector.dump())", "description": "Model that suggests fixes to correct typos.", "default": "d798e898-c6b2-4e39-809f-f502571584e8"}, "topics": {"code": "from sourced.ml.models import Topics\ntopics = Topics().load(%s)\nprint(\"Number of topics:\", len(topics))\nprint(\"Number of tokens:\", len(topics.tokens))", "description": "Topic modeling of Git repositories. All tokens are identifiers extracted from repositories and seen as indicators for topics. They are used to infer the topic(s) of repositories.", "default": "c70a7514-9257-4b33-b468-27a8588d4dfa"}, "bow": {"code": "from sourced.ml.models import BOW\nbow = BOW().load(%s)\nprint(\"Number of documents:\", len(bow))\nprint(\"Number of tokens:\", len(bow.tokens))", "description": "Weighted bag-of-words, that is, every bag is a feature extracted from source code and associated with a weight obtained by applying TFIDF.", "default": "1e3da42a-28b6-4b33-94a2-a5671f4102f4"}, "docfreq": {"code": "from sourced.ml.models import DocumentFrequencies\ndf = DocumentFrequencies().load(%s)\nprint(\"Number of tokens:\", len(df))", "description": "Document frequencies of features extracted from source code, that is, how many documents (repositories, files or functions) contain each tokenized feature.", "default": "f64bacd4-67fb-4c64-8382-399a8e7db52a"}, "id2vec": {"code": "from sourced.ml.models import Id2Vec\nid2vec = Id2Vec().load(%s)\nprint(\"Number of tokens:\", len(id2vec))", "description": "Source code identifier embeddings, that is, every identifier is represented by a dense vector.", "default": "92609e70-f79c-46b5-8419-55726e873cfc"}}}